{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023, Acadential, All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16-11. PyTorch로 구현된 BERT 모델 뜯어보기\n",
    "이번 시간에는 BERT 모델이 어떻게 구현되어 있는지 심도있게 파헤쳐 보는 시간을 가져보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (4.31.0)\n",
      "Requirement already satisfied: datasets in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (2.14.4)\n",
      "Requirement already satisfied: filelock in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jeunghyunbyun/miniconda3/envs/py38/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert import (BertModel,\n",
    "                                      BertConfig,\n",
    "                                      BertForSequenceClassification,\n",
    "                                      BertTokenizer)\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertEmbeddings(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transformers.models.bert.BertForSequenceClassification을 한번 뜯어보자 \n",
    "# 아래 코드는 transformers library에서 구현되어 있는 일부 코드를 발췌한 것임. \n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.models.bert import BertPreTrainedModel, BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    # Init 함수에서는 BertConfig을 입력받습니다.\n",
    "    # BertConfig에서는 Bert model의 구조에 대한 hyperparameter들이 정의되어 있습니다.\n",
    "    # 예: Layer 개수, Hidden dim 크기, Attention head 개수, Dropout rate 등등.\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config) \n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        # BertModel은 Classification Layer가 필요로하는 hidden state를 출력하는 모델입니다.\n",
    "        # Task에 상관없이 공통적으로 사용되는 Backbone architecture입니다.\n",
    "        # 일반적으로 MLM Pre-trained된 모델의 BertModel을 가져와서 이것으로 weight initialize합니다.\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Task specific한 classifier layer\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "    \n",
    "    # Forward pass에서 주의깊게 봐야할 인자들은 input_ids, attention_masks, token_type_ids, labels입니다.\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        \n",
    "        # input_ids: [batch_size, sequence_length] 은 문장을 token id로 변환한 것입니다. \n",
    "        # 저희가 RNN 실습하는 과정에서 tokenize된 문장들이 input_ids로 변환된 것을 이 인자로 입력해 줍니다.\n",
    "        \n",
    "        # attention_mask: [batch_size, sequence_length] 은 padding token을 masking하기 위한 인자입니다.\n",
    "        # attention_mask은 padding token이 아닌 부분은 1, padding token은 0으로 구성됩니다.\n",
    "        \n",
    "        # token_type_ids: [batch_size, sequence_length] 은 문장의 앞뒤를 구분하기 위한 인자입니다.\n",
    "        # token_type_ids는 문장의 앞뒤를 구분하기 위해 앞문장은 0, 뒷문장은 1로 구성됩니다.\n",
    "        \n",
    "        # labels: [batch_size, num_labels] 은 각 문장에 대한 label입니다.\n",
    "        \n",
    "        # 위 값들은 dataset이 tokenizer로 전처리 된 후에 field 값으로 저장되어 있습니다.\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        \n",
    "        # BertModel의 forward pass를 통해 hidden state (output)를 얻습니다.\n",
    "        # outputs: Tuple[torch.FloatTensor] = (last_hidden_state, pooler_output)\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # BERT의 backbone architecture가 어떻게 작동하는지 아래 셀에서 더 자세하게 살펴보겠습니다.\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT 모델 (Backbone architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```BertModel```가 구현된 코드는 Huggingface의 transformers.models.bert.modeling_bert에서 확인할 수 있습니다. \\\n",
    "아래 코드는 ```BertModel```의 구현 코드 중 일부입니다.\n",
    "\n",
    "BertModel을 구성하는 요소들은 다음과 같습니다:\n",
    "1. ```BertEmbeddings```: Token Embedding과 Positional Embedding\n",
    "2. ```BertEncoder```: Encoder Layer를 여러번 쌓은 형태\n",
    "3. ```BertPooler```: Encoder Layer의 마지막 hidden state를 이용해 문장의 특징을 추출\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertEncoder, BertPooler, BaseModelOutputWithPoolingAndCrossAttentions\n",
    "\n",
    "class BertModel(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
    "        \n",
    "        # 아래 method에서 \n",
    "        # attention_mask가 1인 경우 (padding이 아닌 경우),\n",
    "        # extended_attention_mask은 0으로 채워집니다.\n",
    "        # attention_mask가 0인 경우 (padding인 경우),\n",
    "        # extended_attention_mask은 torch.finfo(dtype).min (즉 -inf)으로 채워집니다.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n",
    "            attention_mask,\n",
    "            input_shape\n",
    "        )\n",
    "        \n",
    "        # Embedding\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        # BertEmbedding을 통해서 input_ids를 embedding_output으로 변환해줍니다.\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # BertEncoder를 통해서 embedding_output을 encoder_outputs으로 변환해줍니다.\n",
    "        # BertEncoder가 Classification Layer에서 필요로하는 feature을 추출하는 Backbone Architecture입니다.\n",
    "        \n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "        # BertEncoder은 각 token마다 hidden state를 반환합니다.\n",
    "        # 따라서 문장을 대표하는 feature을 추출하기 위해서 Pooling Layer을 사용합니다.\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertEmbedding\n",
    "\n",
    "Bert Model을 구성하는 3가지 요소 중 하나인 Bert Embedding에 대해 알아보겠습니다.\n",
    "\n",
    "Bert Embedding은 총 3가지로 구성되어 있습니다:\n",
    "1. ```Word Embedding```: 각 단어 (token)을 벡터로 변환\n",
    "2. ```Positional Embedding```: 토큰의 위치 정보를 벡터로 변환\n",
    "3. ```Token Type Embedding```: 토큰의 종류 정보를 벡터로 변환. 토큰의 종류는 총 2가지가 있습니다.\n",
    "    - ```START```, ```END```와 같은 special token\n",
    "    - 일반적인 토큰\n",
    "\n",
    "일반적인 경우에 Word Embedding, Positional Embedding, Token Type Embedding들을 다 더한 것이 Bert Embedding이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Word Embedding\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        \n",
    "        # Position Embedding\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        \n",
    "        # Token Type Embedding\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        # Word Embedding 계산\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        \n",
    "        # Token Type Embedding 계산\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        # Word Embedding + Token Type Embedding\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        \n",
    "        # Position Embedding 계산\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Word Embedding + Token Type Embedding + Position Embedding\n",
    "        embeddings += position_embeddings\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BaseModelOutputWithPastAndCrossAttentions, BertLayer\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # 여러 개의 Bert Layer로 구성되어 있습니다.\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    # BertEncoder는 BertEmbedding에서 출력된 embedding_output을 입력받습니다.\n",
    "    # 즉, hidden_states는 embedding_output입니다.\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        \n",
    "        # BertEncoder는 BertLayer을 차례대로 통과시킵니다.\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            # BertLayer에 대한 forward pass를 실행합니다.\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                layer_head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                past_key_value,\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs[0]  # 각 layer의 출력값을 hidden_states로 정의하여 다음 Layer의 입력값으로 사용합니다.\n",
    "        \n",
    "        # 최종 BertLayer의 hidden_states을 출력하고,\n",
    "        # BertModel에서는 BertPooler에 전달합니다.\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertPooler\n",
    "\n",
    "BertModel의 구성요소들 중 하나입니다. \\\n",
    "BertEncoder에서 출력한 ```hidden_states```를 입력으로 받아서, 대표값을 pooling하는 역할을 합니다. \\\n",
    "Pooling하는 방법은 첫번째 Token에 해당되는 ```hidden_states[:, 0]```를 FC Layer에 통과시켜서 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertLayer\n",
    "\n",
    "Bert Encoder을 구성하는 각 레이어를 구현한 클래스입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertAttention, BertIntermediate, BertOutput\n",
    "from transformers.pytorch_utils import apply_chunking_to_forward\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        \n",
    "        # Self-Attention이 구현되어 있는 부분입니다.\n",
    "        self.attention = BertAttention(config)\n",
    "        \n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        \n",
    "        # Self-Attention이 구현되어 있는 부분입니다.\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertSelfAttention, BertSelfOutput\n",
    "from transformers.modeling_utils import prune_linear_layer, find_pruneable_heads_and_indices\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-Attention을 위한 Layer입니다.\n",
    "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
    "        \n",
    "        # Residual connection, FC Layer, Dropout, LayerNorm으로 구성되어 있습니다.\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \n",
    "        # SelfAttention이 수행되는 부분입니다.\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )  # 아래 Cell을 통해서 Self-attention이 어떻게 구현되어 있는지 살펴보도록 하겠습니다.\n",
    "        \n",
    "        \n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertSelfAttention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x의 shape는 (batch_size, seq_len, hidden_size)이다.\n",
    "        # transpose_for_scores은 x를 (batch_size, seq_len, num_attention_heads, attention_head_size)로 바꾸어준다.\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        \n",
    "        # transpose_for_scores은 각각의 Key, Query, Value 벡터들을 attention_head 개수로 나누어서 Multi-head로 만들어주는 역할을 한다.\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))  # (batch_size, seq_len, num_attention_heads, attention_head_size)\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))  # (batch_size, seq_len, num_attention_heads, attention_head_size)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        # query_layer shape: (batch_size, num_attention_heads, seq_len, attention_head_size)\n",
    "        # key_layer.transpose(...) shape: (batch_size, num_attention_heads, attention_head_size, seq_len)\n",
    "        # attention_scores shape: (batch_size, num_attention_heads, seq_len, seq_len)\n",
    "        # 각 batch, head에 대해서 독립적으로 행렬곱을 수행하는 셈이다.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        # Scaling\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        # Padding에 해당되는 token들의 attention_mask은 -inf로 만들어주었다.\n",
    "        # 따라서 softmax를 통해 attention_probs를 구할 때, padding에 해당되는 token들은 0에 가까운 값이 나오게 된다.\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        # Attention weight (probability)를 value_layer에 곱해준다.\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
