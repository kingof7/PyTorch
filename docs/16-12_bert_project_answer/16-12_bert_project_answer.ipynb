{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023, Acadential, All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16-12. BERT 모델을 활용한 NLP 프로젝트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/98/46/f6a79f944d5c7763a9bc13b2aa6ac72daf43a6551f5fb03bccf0a9c2fec1/transformers-4.33.3-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m715.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m761.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/03/5e/9a4cabe86a3b4e67bd2cf795a2e84de01c735c8c1c1d88795425847ccbbe/regex-2023.8.8-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/34/0e/12d55d5dd648b8f7ea7216c5b7cef9703b4dbd3b2a042872c711d5e98551/safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=8.0.0 from https://files.pythonhosted.org/packages/f6/c8/ad19a273d6e825fcd040f51b559d30f55d652d82f5badf1f546a9a06aeb9/pyarrow-13.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pyarrow-13.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from datasets) (2.1.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/ee/23/020ff3fa540e0d06886b6b866f1e173c554723e04f286ac205c5ddeb479e/xxhash-3.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading xxhash-3.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/e7/41/96ac938770ba6e7d5ae1d8c9cafebac54b413549042c6260f0d0a6ec6622/multiprocess-0.70.15-py311-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets)\n",
      "  Obtaining dependency information for fsspec[http]<2023.9.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/47/10/33abd984a476e314afdb4711fbd0aac1b25927676fa591445537da3aee98/aiohttp-3.8.5-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp311-cp311-macosx_11_0_arm64.whl (61 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/9d/8b/8ab8143541b2c5fff4189fad7853e61d30e4ec4749ebf91e1d598c4e7c57/frozenlist-1.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jeunghyunbyun/miniconda3/envs/py311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m333.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m448.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m442.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Downloading aiohttp-3.8.5-cp311-cp311-macosx_11_0_arm64.whl (339 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m339.6/339.6 kB\u001b[0m \u001b[31m400.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-13.0.0-cp311-cp311-macosx_11_0_arm64.whl (23.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.8.8-cp311-cp311-macosx_11_0_arm64.whl (289 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl (406 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.9/406.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.3.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp311-cp311-macosx_11_0_arm64.whl (46 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: tokenizers, safetensors, xxhash, regex, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 dill-0.3.7 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.17.3 multidict-6.0.4 multiprocess-0.70.15 pyarrow-13.0.0 regex-2023.8.8 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.3 xxhash-3.3.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "# Install transformers, datasets libraries\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Drive을 Colab에 Mount하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "os.chdir(\"/content/drive/MyDrive/Lesson/인프런 강의 - 딥러닝 이론 실무 완전 정복/practicals/section_16/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert import (BertModel,\n",
    "                                      BertConfig,\n",
    "                                      BertForSequenceClassification,\n",
    "                                      BertTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Dataset\n",
    "\n",
    "RNN 실습에서 구현하였던 코드를 사용해서 IMDB 데이터셋을 불러오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import get_imdb_dataset, get_tokenizer, tokenize_dataset, get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6adbe41b698d4e5c9559d7083e5d9361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb42c8ae82d74f7683e3e9ab18e100a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa83d9fb6c04d4db39069efe86e4914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset 불러오기\n",
    "dataset = get_imdb_dataset()\n",
    "\n",
    "# Tokenizer 생성\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# Tokenizer를 이용해 Dataset을 Tokenize\n",
    "tokenized_dataset = tokenize_dataset(dataset, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = get_dataloader(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 모델 정의하기\n",
    "IMDB 데이터셋은 Sentiment Analysis로 Sequence Classification task의 한 종류입니다. \\\n",
    "따라서 Sequence Classification 용 BERT 모델을 정의하고, 이를 학습시켜보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "sample_data = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids=sample_data[\"input_ids\"],\n",
    "               attention_mask=sample_data[\"attention_mask\"],\n",
    "               token_type_ids=sample_data[\"token_type_ids\"],\n",
    "               labels=sample_data[\"label\"],\n",
    "               return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.SequenceClassifierOutput"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7069, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0490,  0.2314],\n",
       "        [-0.1440,  0.2051],\n",
       "        [-0.0839,  0.1474],\n",
       "        [ 0.0355,  0.1337],\n",
       "        [-0.1383,  0.1994],\n",
       "        [-0.1206,  0.1635],\n",
       "        [-0.2156,  0.2050],\n",
       "        [-0.0634,  0.1834],\n",
       "        [-0.0970,  0.1848],\n",
       "        [-0.0833,  0.1902],\n",
       "        [-0.1903,  0.1976],\n",
       "        [-0.1709,  0.1961],\n",
       "        [-0.2238,  0.1675],\n",
       "        [-0.0439,  0.1164],\n",
       "        [-0.1793,  0.1604],\n",
       "        [-0.0841,  0.1478]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import torch \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "best_test_acc = 0\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    correct = 0 \n",
    "    model.train()\n",
    "    size = len(train_dataloader.dataset) \n",
    "    tbar = tqdm(train_dataloader)\n",
    "    \n",
    "    for batch in tbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        output = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=label,\n",
    "                    return_dict=True)\n",
    "        \n",
    "        loss = output.loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tbar.set_description(f\"loss - {loss.item():.3f}\")\n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "        pred = output.logits \n",
    "        \n",
    "        correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "\n",
    "    correct /= size\n",
    "    train_acc_list.append(correct)\n",
    "\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    \n",
    "    tbar = tqdm(test_dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in tbar:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "            output = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=label,\n",
    "                        return_dict=True)\n",
    "            loss = output.loss \n",
    "            pred = output.logits \n",
    "        \n",
    "            correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "            tbar.set_description(f'loss - {loss.item():.3f}')\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    test_loss_list.append(test_loss)\n",
    "    test_acc_list.append(correct)\n",
    "    print(f\"test loss   {test_loss}\")\n",
    "    print(f\"test acc   {correct}\")\n",
    "    \n",
    "    if correct > best_test_acc:\n",
    "        best_test_acc = correct\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        torch.save(model.state_dict(), 'checkpoints/best_bert_model_pretrained.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(train_loss_list)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"BERT (pretrained) - Train Loss vs Iteration\")\n",
    "plt.savefig('figures/bert_train_loss_pretrained.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_acc_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "plt.title(\"BERT (pretrained) - Train accuracy vs epochs\")\n",
    "plt.savefig('figures/bert_train_acc_pretrained.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(test_loss_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Test Loss\")\n",
    "plt.title(\"BERT (pretrained) - Test Loss vs epochs\")\n",
    "plt.savefig('figures/bert_test_loss_pretrained.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(test_acc_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"BERT (pretrained) - Test accuracy vs epochs\")\n",
    "plt.savefig('figures/bert_test_acc_pretrained.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_summary = {\"bert_pretrained_\": train_loss_list}\n",
    "train_acc_summary = {\"bert_pretrained\": train_acc_list}\n",
    "test_loss_summary = {\"bert_pretrained\": test_loss_list}\n",
    "test_acc_summary = {\"bert_pretrained\": test_acc_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_loss_summary).to_csv('results/train_loss_summary.csv')\n",
    "pd.DataFrame(train_acc_summary).to_csv('results/train_acc_summary.csv')\n",
    "pd.DataFrame(test_loss_summary).to_csv('results/test_loss_summary.csv')\n",
    "pd.DataFrame(test_acc_summary).to_csv('results/test_acc_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly Initialized BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "# Initializing a BERT bert-base-uncased style configuration\n",
    "configuration = BertConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the bert-base-uncased style configuration\n",
    "model = BertForSequenceClassification(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_val import train_loop, val_loop\n",
    "import torch \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "best_test_acc = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_loop(model, train_dataloader, optimizer)\n",
    "    train_loss_list.extend(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    \n",
    "    test_loss, test_acc = val_loop(model, test_dataloader)\n",
    "    print(f\"test loss   {test_loss}\")\n",
    "    print(f\"test acc   {test_acc}\")\n",
    "    \n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        torch.save(model.state_dict(), 'checkpoints/best_bert_model_random.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_summary[\"bert_random\"] = train_loss_list\n",
    "train_acc_summary[\"bert_random\"] = train_acc_list\n",
    "test_loss_summary[\"bert_random\"] = test_loss_list\n",
    "test_acc_summary[\"bert_pretrained\"] = test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_loss_summary).to_csv('results/train_loss_summary.csv')\n",
    "pd.DataFrame(train_acc_summary).to_csv('results/train_acc_summary.csv')\n",
    "pd.DataFrame(test_loss_summary).to_csv('results/test_loss_summary.csv')\n",
    "pd.DataFrame(test_acc_summary).to_csv('results/test_acc_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(train_loss_list)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"BERT (random) - Train Loss vs Iteration\")\n",
    "plt.savefig('figures/bert_train_loss_random.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_acc_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "plt.title(\"BERT (random) - Train accuracy vs epochs\")\n",
    "plt.savefig('figures/bert_train_acc_random.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(test_loss_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Test Loss\")\n",
    "plt.title(\"BERT (random) - Test Loss vs epochs\")\n",
    "plt.savefig('figures/bert_test_loss_random.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(test_acc_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"BERT (pretrained) - Test accuracy vs epochs\")\n",
    "plt.savefig('figures/bert_test_acc_random.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between BERT pretrained vs. BERT random initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_acc_summary[\"bert_random\"], label=\"Random\")\n",
    "plt.plot(train_acc_summary[\"bert_pretrained\"], label=\"Pretrained\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"BERT - Train accuracy vs epochs\")\n",
    "plt.savefig('figures/bert_train_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(test_loss_summary[\"bert_random\"], label=\"Random\")\n",
    "plt.plot(test_loss_summary[\"bert_pretrained\"], label=\"Pretrained\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Test Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"BERT - Test Loss vs epochs\")\n",
    "plt.savefig('figures/bert_test_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(test_acc_summary[\"bert_random\"], label=\"Random\")\n",
    "plt.plot(test_acc_summary[\"bert_pretrained\"], label=\"Pretrained\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"BERT - Test accuracy vs epochs\")\n",
    "plt.savefig('figures/bert_test_acc.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
