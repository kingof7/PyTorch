{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023, Acadential, All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4becd7d9-f098-4af5-9990-2055683484f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6-8. Pytorch로 구현하는 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbed7e-fe6b-4d93-ba7f-367ef7020243",
   "metadata": {},
   "source": [
    "## Automatic Differentiation: pytorch의 backpropagation\n",
    "### (각 layer의 gradient에 대한 계산)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c49f1a-cfec-4b7a-86d8-393ffecd7b55",
   "metadata": {},
   "source": [
    "- PyTorch에서는 built-in differentiation engine인 `torch.autograd`을 통해서 Backpropagation을 수행할 수 있습니다.\n",
    "- 이를 통하여 model weight (parameter)들에 대한 `Loss Gradient`을 구할 수 있습니다.\n",
    "- 예를 들어서 저희가 `PyTorch Module Class`의 `forward` 함수를 실행하면\n",
    "     - 해당 함수를 구성하는 연산들이 차례대로 수행되면서\n",
    "     - 해당 연산들에 대한 `Computational Graph`가 생성됩니다.\n",
    "- 이렇게 정의된 `Computation Graph`에 대해서 backpropagation을 수행하여 gradient을 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccca960-d1df-46c8-89a6-4a77d3249a22",
   "metadata": {},
   "source": [
    "## Computation Graph의 예시\n",
    "\n",
    "다음과 같은 computational graph가 있다고 가정해보겠습니다. \n",
    "\n",
    "- input = x \n",
    "- parameter = W, b \n",
    "- activation = tanh \n",
    "- loss function = MSE Loss \n",
    "\n",
    "```\n",
    "L = L(y_gt, y_pred)\n",
    "y_pred = activation(W @ x + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65c458f-a81c-4c17-af4a-345cb9cc8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(10)\n",
    "y = torch.zeros(5)\n",
    "W = torch.rand(10, 5, requires_grad=True)\n",
    "b = torch.rand(5, requires_grad=True)\n",
    "h = torch.matmul(x, W) + b\n",
    "h = torch.tanh(h)\n",
    "loss = torch.sum(torch.square(y - h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a2fbe-a16a-42c4-b926-9b9a1279f7da",
   "metadata": {},
   "source": [
    "## Computing Gradients\n",
    "\n",
    "- ```Output.backward()```을 실행하면 ```Output tensor```을 각 ```parameter```에 대해서 미분한 경사들을 자동으로 계산해줍니다.\n",
    "- ```d(Output)/d(tensor)```은 ```tensor.grad```에 누적됩니다.\n",
    "- 만약에 기존에 ```.backward```로 계산된 Loss가 있다면 새로운 gradient는 기존의 gradient에 더해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765292f0-683c-4a33-a7b3-a9fe3a0dd4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0036, 0.0014, 0.0012, 0.0020, 0.0105],\n",
      "        [0.0672, 0.0269, 0.0217, 0.0365, 0.1970],\n",
      "        [0.0043, 0.0017, 0.0014, 0.0023, 0.0125],\n",
      "        [0.0423, 0.0169, 0.0136, 0.0230, 0.1241],\n",
      "        [0.0759, 0.0304, 0.0245, 0.0412, 0.2225],\n",
      "        [0.0869, 0.0347, 0.0280, 0.0472, 0.2547],\n",
      "        [0.0177, 0.0071, 0.0057, 0.0096, 0.0518],\n",
      "        [0.0581, 0.0232, 0.0187, 0.0316, 0.1704],\n",
      "        [0.0442, 0.0177, 0.0142, 0.0240, 0.1296],\n",
      "        [0.0238, 0.0095, 0.0077, 0.0129, 0.0699]])\n",
      "tensor([0.0976, 0.0390, 0.0314, 0.0530, 0.2860])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(W.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8920f-e0a7-4534-bc87-292e420b0ef5",
   "metadata": {},
   "source": [
    "## 참고 사항:\n",
    "- PyTorch에서 by default로 ```backward(gradient=None)```은 scalar value에 대해서만 수행할 수 있습니다!\n",
    "- 물론 Vector, Matrix, 혹은 Tensor에 대해서도 backward을 수행해 줄 수는 있지만 이때는 별도로, 각 element들에 대한 Gradient을 명시해줘야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41662ba-4bc6-47c8-a636-aef01ca5e510",
   "metadata": {},
   "source": [
    "### 에러 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26b98e5-1ecf-4da9-a226-506b192af80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss shape ==  torch.Size([5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msquare(y \u001b[38;5;241m-\u001b[39m h)  \u001b[38;5;66;03m# Raises an error! Because loss is not scalar\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss shape == \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/autograd/__init__.py:193\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    192\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 193\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/autograd/__init__.py:88\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10)  # input tensor\n",
    "y = torch.zeros(5)  # expected output\n",
    "W = torch.randn(10, 5, requires_grad=True)\n",
    "b = torch.randn(5, requires_grad=True)\n",
    "h = torch.matmul(x, W)+b\n",
    "h = torch.tanh(h)\n",
    "loss = torch.square(y - h)  # Raises an error! Because loss is not scalar\n",
    "print(\"loss shape == \", loss.shape)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a9adb-32e1-4ae3-81b3-9d1cb8b2c0a2",
   "metadata": {},
   "source": [
    "### (앞서 살펴본) 잘 작동되는 예시\n",
    "\n",
    "torch.sum을 통해서 loss을 scalar 값으로 구했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60587cab-97d0-4f10-adab-ec4dad61cb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss shape ==  torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10)  # input tensor\n",
    "y = torch.zeros(5)  # expected output\n",
    "W = torch.randn(10, 5, requires_grad=True)\n",
    "b = torch.randn(5, requires_grad=True)\n",
    "h = torch.matmul(x, W)+b\n",
    "h = torch.tanh(h)\n",
    "loss = torch.sum(torch.square(y - h))\n",
    "print(\"loss shape == \", loss.shape) \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19291773-b23d-4a46-8351-0ed530abf5ca",
   "metadata": {},
   "source": [
    "## Disabling Gradient Tracking\n",
    "\n",
    "PyTorch에서는 by default로 backward propagation을 수행하는데 필요한 값(각 layer에서의 intermediate values)들을 저장해둡니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac075dae-4faf-4ce9-9e40-c05c2ba1a98f",
   "metadata": {},
   "source": [
    "d(Leaf)/d(Root) gradient을 구하기 위해서 Leaf (Descendent node)로부터 Root node까지 backpropagation하려면:\n",
    "1. 각 layer의 output값\n",
    "2. 각 layer의 input 값\n",
    "\n",
    "들을 저장해둬야합니다 (gradient을 구하기 위해 필요한 값).\n",
    "\n",
    "하지만 만약에 training 단계가 아니라 inference (혹은 evaluation)단계의 경우 backpropagation이 필요하지 않습니다.\n",
    "\n",
    "따라서 inference의 경우 각 layer의 output값과 input값을 굳이 저장할 필요가 없고 Gradient을 계산할 필요도 없습니다.\n",
    "\n",
    "그러므로 inference에서는 torch.no_grad()을 사용해서 Gradient에 대한 tracking을 disable합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96428538-e5ae-422a-b3fb-d03eb3873cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, W)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, W)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9f541-108c-4845-9d4f-dbf0f5129826",
   "metadata": {},
   "source": [
    "### To detach a tensor from computational graph (disabling gradient computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7228d81-73a3-4d63-822b-9c22a73b082f",
   "metadata": {},
   "source": [
    "또 간혹 pre-trained된 모델을 가져와서 finetuning하는데 사용할때 layer 전부 다 학습하기보다 일부 (예를 들어 feature extractor)의 parameter들은 고정된 상태 (freeze)로 학습을 하고 싶은 경우가 있을 수 있습니다.\n",
    "\n",
    "이 때는 freeze할 weight parameter들에 대해서는 .detach()해서 gradient에 대해서는 gradient tracking을 disable시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99de3517-2adf-48fa-a272-f58761830714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, W)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fdec29-a588-44dd-b926-3cc9ffe061ff",
   "metadata": {},
   "source": [
    "## Zeroing the gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c735e-01fd-4c81-900e-7a7b54dfa899",
   "metadata": {},
   "source": [
    ".backward()을 여러번 실행하게 되면 해당 tensor에 대한 gradient은 \"여러번 쌓이게\" 됩니다! \\\n",
    "즉, 기존에 계산한 gradient에 더해집니다. \\\n",
    "따라서 의도된 것이 아니라면, gradient을 \"zero\" (비워줘야) 해당 oepration에 대한 gradient을 구할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba87e15-d3e3-4b47-8034-dc8b7f1327c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)  # operation == (x+1) ** 2 -> gradient == 2(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f083cd-5770-47c0-b41d-621334c6e41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]], requires_grad=True)\n",
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Gradient after zeroing\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)  # operation == (x+1) ** 2 -> gradient == 2(x+1)\n",
    "print(inp)\n",
    "\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "\n",
    "inp.grad.zero_()\n",
    "print(f\"\\nGradient after zeroing\\n{inp.grad}\")\n",
    "\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832d997-0f9c-41fc-b4d7-1608da08e7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
